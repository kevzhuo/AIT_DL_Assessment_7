{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG + LLM Assessment"
      ],
      "metadata": {
        "id": "e1xCp6n57BB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ],
      "metadata": {
        "id": "qmI3jBxK68-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documents or,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ],
      "metadata": {
        "id": "K3STgq_s6_mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documents and Prompts"
      ],
      "metadata": {
        "id": "P9po1WgPwPXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topic that I plan to serve as the knowledge base for my RAG system are latest scientific papers from arxiv. The model that I am using (Llama 2) was trained beteween Janurary 2023 and July 2023, so the selected documents that I have chosen have been created after then. The documents are all from 2024 and are April Fools documents so there are for sure no replicates of this in the original training set.\n",
        "\n",
        "https://arxiv.org/pdf/2403.19993\n",
        "\n",
        "https://arxiv.org/pdf/2403.19749\n",
        "\n",
        "https://arxiv.org/pdf/2403.20219\n",
        "\n",
        "https://arxiv.org/pdf/2403.20143\n",
        "\n",
        "https://arxiv.org/pdf/2403.20314\n",
        "\n",
        "The promps that I plan to use are:\n",
        "1. Why is FLAMINGO is the perfect name for an array of Cherenkov telescopes? (REAL)\n",
        "2. How do different messengers affect a person's personality? (REAL)\n",
        "3. What is the Universe of the birthday achieved by solving the Einstein-Boltzmann equations using CLASS? (REAL)\n",
        "4. What is Elon Musk's Goldfish name? (FAKE)"
      ],
      "metadata": {
        "id": "HfGxCZi9qFPn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPpiHVgj4CmG"
      },
      "source": [
        "## Installing dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zjLkm65J_S0v"
      },
      "outputs": [],
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null\n",
        "!pip install langchain > null\n",
        "!pip install chromadb > null\n",
        "!pip install sentence_transformers > null # ==2.2.2\n",
        "!pip install unstructured > null\n",
        "!pip install pdf2image > null\n",
        "!pip install pdfminer.six > null\n",
        "!pip install unstructured-pytesseract > null\n",
        "!pip install unstructured-inference > null\n",
        "!pip install faiss-gpu > null\n",
        "!pip install pikepdf > null\n",
        "!pip install pypdf > null\n",
        "!pip install accelerate > null\n",
        "!pip install pillow_heif > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30AIKqF9rTuv"
      },
      "source": [
        "Important\n",
        "------\n",
        "Restart the kernel after installing the packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "qEb7_aQ50Y-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X22Ccat1oXE2"
      },
      "source": [
        "# Imports\n",
        "Next, we import the necessary Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PatG1bZXGOws",
        "outputId": "e61b29b7-7958-4490-986d-18e62fee612c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# you need to define your private User Access Token from Huggingface\n",
        "# to be able to access models with accepted licence\n",
        "HUGGINGFACE_UAT=\"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\"\n",
        "login(HUGGINGFACE_UAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N_hQ_IfuoWT6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "9a7b212bf2244145b7991422c90f2111",
            "41b6a182ae9d4153943906730879ddb5",
            "a5c839d119cc4a618acf72e2d6aaea5e",
            "af9976b075ee4427b14879f4a5db805a",
            "a9fa95d9c681459b8054c9b1753e5e92",
            "31fb51ce927c40f98ad5958a27ff431c",
            "ae3676bda65f4c65aef86d4354be7a8e",
            "23da7bef970748f0a2f62e092d14e20b",
            "aeb457a7f0e34e83971e5058c9daac1b",
            "a6b09d1c24a94402bd542d78fa615bf4",
            "91b1aca63f3748ce83a4902b7f026298"
          ]
        },
        "outputId": "17f78b3d-c7db-4146-d8cb-2f6271f3c75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a7b212bf2244145b7991422c90f2111"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"google/gemma-2b-it\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0 # For RAG we would like to have determenistic answers\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BX6bnnL45a2N"
      },
      "outputs": [],
      "source": [
        "template_gemma = \"\"\"\n",
        "user\n",
        "{text}\n",
        "model\n",
        "\"\"\"\n",
        "\n",
        "template_llama2 = \"\"\"\n",
        "user\n",
        "{text}\n",
        "model\n",
        "\"\"\"\n",
        "\n",
        "if \"llama\" in model_name:\n",
        "  template=template_llama2\n",
        "else:\n",
        "  template=template_gemma\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Why is FLAMINGO is the perfect name for an array of Cherenkov telescopes?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))\n",
        "print(\"\\n\")\n",
        "\n",
        "text = \"How do different messengers affect a person's personality?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))\n",
        "print(\"\\n\")\n",
        "\n",
        "text = \"What is the Universe of the birthday achieved by solving the Einstein-Boltzmann equations using CLASS?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))\n",
        "print(\"\\n\")\n",
        "\n",
        "text = \"What is Elon Musk's Goldfish name?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuuTfrBr2FOV",
        "outputId": "2a48e899-c66d-4597-be14-65c43a2e53ee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user Why is FLAMINGO is the perfect name for an array of Cherenkov telescopes? model The perfect\n",
            "name for an array of Cherenkov telescopes is FLAMINGO.\n",
            "\n",
            "\n",
            "user How do different messengers affect a person's personality? model The personality of a person is\n",
            "affected by various messengers, including:  * ****Personal communication:** Personal communication\n",
            "involves direct interactions between a person and others, such as through phone calls, emails, and\n",
            "face-to-face conversations. * ****Media:** Media, such as television, radio, and social media,\n",
            "provides a platform for people to share information and interact with each other. *\n",
            "****Technology:** Technology, such as smartphones, laptops, and social media platforms, facilitates\n",
            "communication and allows people to connect with others in various ways. * ****Social groups:**\n",
            "Social groups, such as clubs, organizations, and communities, provide opportunities for people to\n",
            "engage with others and participate in shared activities. * ****Cultural influences:** Cultural\n",
            "influences shape the way people communicate and interact with each other, including through\n",
            "language, traditions, and values.  By interacting with these messengers, people can develop and\n",
            "maintain their personalities, learn about others, and create meaningful connections with others.\n",
            "\n",
            "\n",
            "user What is the Universe of the birthday achieved by solving the Einstein-Boltzmann equations using\n",
            "CLASS? model The Universe of the birthday achieved by solving the Einstein-Boltzmann equations using\n",
            "CLASS is a vast and infinite space that encompasses all of the known and unknown entities. It is a\n",
            "place where everything is possible, including the formation of new galaxies, the existence of exotic\n",
            "particles, and the ultimate fate of the universe itself.\n",
            "\n",
            "\n",
            "user What is Elon Musk's Goldfish name? model This information is not provided in the context.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2SNJZEtlDo1"
      },
      "source": [
        "## RAG on the web\n",
        "In this section, we download content from the internet, vectorise it and store the vectors, then search these vectors and generate the answer using the associated text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "z8Y3YnRjJGEL"
      },
      "outputs": [],
      "source": [
        "web_loader = UnstructuredURLLoader(\n",
        "    urls=[\"https://arxiv.org/pdf/2403.19993\", \"https://arxiv.org/pdf/2403.19749\",\n",
        "          \"https://arxiv.org/pdf/2403.20219\", \"https://arxiv.org/pdf/2403.20143\",\n",
        "          \"https://arxiv.org/pdf/2403.20314\"], mode=\"elements\", strategy=\"fast\"\n",
        "    )\n",
        "web_doc = web_loader.load()\n",
        "updated_web_doc = filter_complex_metadata(web_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVjSIuPMmYvX",
        "outputId": "ffc6b7fd-b255-4daf-b95a-2283bc2ff26a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "881"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
        "chunked_web_doc = text_splitter.split_documents(updated_web_doc)\n",
        "len(chunked_web_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R3hqEWvUcyBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67f5f46-0e17-4ca7-ad18-d93dbce0427d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings() # default model_name=\"sentence-transformers/all-mpnet-base-v2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA3xtLSQmh-v",
        "outputId": "d85d00cf-a3d4-448b-e456-c3512196501a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.9 s, sys: 9.84 ms, total: 2.91 s\n",
            "Wall time: 2.91 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Create the vectorized db with FAISS\n",
        "\n",
        "db_web = FAISS.from_documents(chunked_web_doc, embeddings)\n",
        "\n",
        "# Create the vectorized db with Chroma\n",
        "# from langchain.vectorstores import Chroma\n",
        "# db_web = Chroma.from_documents(chunked_web_doc, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxfNkw_PnItp",
        "outputId": "32808246-d705-4dab-c237-bea75fdc7c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 9.06 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "prompt_template_gemma = \"\"\"\n",
        "user\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}<end_of_turn>\n",
        "\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "prompt_template_llama2 = \"\"\"\n",
        "system\n",
        "\n",
        "Use the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}user\n",
        "\n",
        "{question}assistant\n",
        "\"\"\"\n",
        "\n",
        "if \"llama\" in model_name:\n",
        "  prompt_template=prompt_template_llama2\n",
        "else:\n",
        "  prompt_template=prompt_template_gemma\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_web = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_web.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n"
      ],
      "metadata": {
        "id": "p1S7nQqbgLRl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj-yQeYIH209",
        "outputId": "9b58eea7-e343-4856-ed20-c5bb1cc1eb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user Use the following context to Answer the question at the end. Do not use any other information.\n",
            "If you can't find the relevant information in the context, just say you don't have enough\n",
            "information to answer the question. Don't try to make up an answer.  Why FLAMINGO is the perfect\n",
            "name for an array of Cherenkov telescopes  Overall, using the name FLAMINGO for an array of\n",
            "Cherenkov telescopes can help to create a more inclu- sive and diverse community of astronomers and\n",
            "astro- physicists, promoting a more equitable and representa- tive field.  Finally, the name\n",
            "FLAMINGO can help to make ar- rays of Cherenkov telescopes more accessible and ap- proachable to\n",
            "people who may not have a background in astrophysics. By using a name that is fun and easy to\n",
            "remember, the project can help to break down barri-  In this paper, we have argued that FLAMINGO is\n",
            "the perfect name for an array of Cherenkov telescopes for several reasons. Firstly, the color pink,\n",
            "which is as- sociated with flamingos, has been shown to be the most suitable color for Cherenkov\n",
            "telescopes, providing the best characteristics to enhance sensitivity. Secondly, the keen visual\n",
            "ability of flamingos aligns well with the pri- mary purpose of Cherenkov telescopes to detect faint\n",
            "signals of high-energy cosmic rays and gamma rays. In addition, the name FLAMINGO has a fun and mem-\n",
            "orable quality that can help to increase public engage- ment and interest in the field of\n",
            "astrophysics. Using a name like FLAMINGO can also help to address diver- sity issues in the field of\n",
            "astrophysics by creating a more inclusive and welcoming community of astronomers and\n",
            "astrophysicists.  In conclusion, the name FLAMINGO represents a powerful branding strategy for any\n",
            "next-generation ar- ray of Cherenkov Telescopes, capturing the essence of the scientific goals of\n",
            "the project while creating a fun and approachable image. By using the name FLAMINGO and associated\n",
            "branding, the project can generate ex- citement and engagement among the public while also inspiring\n",
            "and supporting a more diverse flock of as- tronomers and astrophysicists. Visit https://flamingo.\n",
            "science. to stay up to date on the ongoing efforts of the FLAMINGO collaboration.  Question: Why is\n",
            "FLAMINGO is the perfect name for an array of Cherenkov telescopes?<end_of_turn>\n",
            "<start_of_turn>model The name FLAMINGO is the perfect name for an array of Cherenkov telescopes\n",
            "because it captures the essence of the scientific goals of the project while creating a fun and\n",
            "approachable image.\n",
            "\n",
            " \n",
            "\n",
            "user Use the following context to Answer the question at the end. Do not use any other information.\n",
            "If you can't find the relevant information in the context, just say you don't have enough\n",
            "information to answer the question. Don't try to make up an answer.  In this section, we cover the\n",
            "effects of different messen- gers on one’s personality. We go over the known messen- gers that we\n",
            "have observed in the past either as persistent sources or transients.  Cosmic rays are the only\n",
            "messenger with electric charge, and they therefore dictate your “electronic per- sonality”: the\n",
            "personality you project through text, so- cial media, and email for those older than z = 3 × 10−9.\n",
            "This electronic personality does not necessarily match the personality dictated by other messengers.\n",
            "For exam- ple, when the authors of this paper meet online friends in person, these friends are often\n",
            "upset that we are not “fun to be around” or “six-foot-two” like our profiles pro- claim. Although\n",
            "this was previously attributed to lying and insecurity, we can now see that it stems from a differ-\n",
            "ence in our cosmic ray astrological signs and our visible astrological signs.  Multi-Messenger\n",
            "Astrology  While this paper is not intended to describe the as- trological properties of messengers\n",
            "in an exhaustive way, it has left us exhausted, so we leave further analyses to smarter, harder-\n",
            "working individuals. In the meantime, the larger astrological society should be cognizant of the\n",
            "II. MULTI-MESSENGER SIGNALS  Question: How do different messengers affect a person's\n",
            "personality?<end_of_turn>  <start_of_turn>model Different messengers affect a person's personality\n",
            "by projecting different personality traits through text, email, and social media. These different\n",
            "personality traits are derived from the different astrological signs associated with each messenger.\n",
            "\n",
            " \n",
            "\n",
            "user Use the following context to Answer the question at the end. Do not use any other information.\n",
            "If you can't find the relevant information in the context, just say you don't have enough\n",
            "information to answer the question. Don't try to make up an answer.  To constrain the value of π\n",
            "using cosmological ob- servations, we have modified the cosmological Einstein- Boltzmann solver\n",
            "CLASS [19] to take π as an input pa- rameter. Our version of CLASS is publicly available at\n",
            "https://github.com/EBHolm/class_public/tree/ varying-pi. Hence, the interested reader may try out\n",
            "the code for themselves, and even implement different values of π in their own, favourite model.\n",
            "The best-fit cosmology found from the baseline MCMC in the previous section is shown in Table I. By\n",
            "solv- ing the Einstein-Boltzmann equations using CLASS for these values, we find the age of the\n",
            "Universe to be 1022 nautical miles at the time of the pub- 6.712314 lication of this paper, i.e.\n",
            "April 1, 2024. By noting that on average, there are 365.242374 days in a year, we find that the age\n",
            "4015.63 Mpc corresponds to 13,097,255,367 years and 132 days ago from today. In order to find the\n",
            "correct birthday of the Universe, we must extrapolate our calendar back in time and account for all\n",
            "the leap years. There are three rules regarding leap years:  π enters the Einstein-Boltzmann\n",
            "equations in numer- ous places. Examples include the Fermi-Dirac initial distribution for neutrinos,\n",
            "the commonly occurring 8πG constant, primordial inflation potentials, changes to the Doppler\n",
            "corrections of the KHe term in the thermo- dynamics, in the computation of the Gauss-Legendre\n",
            "quadrature weights [12], and in the computation of the hyperspherical Bessel functions [20].\n",
            "Furthermore, a non- π value of π alters the definition of the Fourier transform, inherently\n",
            "impacting the differential equations solved and the final power spectrum quantities computed.  Our\n",
            "implementation of π in the Einstein-Boltzmann solver CLASS is rather rudimentary. For example, we\n",
            "did not rescale the Bessel functions, which have roots at π and are integral to the computation of\n",
            "the Cℓ coefficients, we did not take the effect on reaction rates during BBN into account, and we\n",
            "also did not rescale the periods of the trigonometric functions; ideally, one would rescale them as\n",
            "IMPLEMENTATION AND EFFECTS ON COSMOLOGICAL OBSERVABLES  Question: What is the Universe of the\n",
            "birthday achieved by solving the Einstein-Boltzmann equations using CLASS?<end_of_turn>\n",
            "<start_of_turn>model The Universe of the birthday achieved by solving the Einstein-Boltzmann\n",
            "equations using CLASS is 1022 nautical miles at the time of the publication of this paper, i.e.\n",
            "April 1, 2024.\n",
            "\n",
            " \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.2\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user Use the following context to Answer the question at the end. Do not use any other information.\n",
            "If you can't find the relevant information in the context, just say you don't have enough\n",
            "information to answer the question. Don't try to make up an answer.    Question: What is Elon Musk's\n",
            "Goldfish name?<end_of_turn>  <start_of_turn>model The context does not provide information about\n",
            "Elon Musk's Goldfish name.\n"
          ]
        }
      ],
      "source": [
        "query = \"Why is FLAMINGO is the perfect name for an array of Cherenkov telescopes?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "query = \"How do different messengers affect a person's personality?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "query = \"What is the Universe of the birthday achieved by solving the Einstein-Boltzmann equations using CLASS?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "query = \"What is Elon Musk's Goldfish name?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a7b212bf2244145b7991422c90f2111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41b6a182ae9d4153943906730879ddb5",
              "IPY_MODEL_a5c839d119cc4a618acf72e2d6aaea5e",
              "IPY_MODEL_af9976b075ee4427b14879f4a5db805a"
            ],
            "layout": "IPY_MODEL_a9fa95d9c681459b8054c9b1753e5e92"
          }
        },
        "41b6a182ae9d4153943906730879ddb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31fb51ce927c40f98ad5958a27ff431c",
            "placeholder": "​",
            "style": "IPY_MODEL_ae3676bda65f4c65aef86d4354be7a8e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a5c839d119cc4a618acf72e2d6aaea5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23da7bef970748f0a2f62e092d14e20b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aeb457a7f0e34e83971e5058c9daac1b",
            "value": 2
          }
        },
        "af9976b075ee4427b14879f4a5db805a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6b09d1c24a94402bd542d78fa615bf4",
            "placeholder": "​",
            "style": "IPY_MODEL_91b1aca63f3748ce83a4902b7f026298",
            "value": " 2/2 [00:20&lt;00:00,  8.78s/it]"
          }
        },
        "a9fa95d9c681459b8054c9b1753e5e92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31fb51ce927c40f98ad5958a27ff431c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae3676bda65f4c65aef86d4354be7a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23da7bef970748f0a2f62e092d14e20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeb457a7f0e34e83971e5058c9daac1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6b09d1c24a94402bd542d78fa615bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91b1aca63f3748ce83a4902b7f026298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}