{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG + LLM Assessment"
      ],
      "metadata": {
        "id": "e1xCp6n57BB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
      ],
      "metadata": {
        "id": "qmI3jBxK68-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
        "   * latest scientific papers from arxiv.org,\n",
        "   * fiction books released,\n",
        "   * legal documents or,\n",
        "   * social media posts.\n",
        "\n",
        "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
        "\n",
        "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
        "\n",
        "3. Load an LLM with at least 5B parameters. (10 points)\n",
        "\n",
        "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
        "\n",
        "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
        "\n",
        "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
      ],
      "metadata": {
        "id": "K3STgq_s6_mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documents and Prompts"
      ],
      "metadata": {
        "id": "P9po1WgPwPXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topic that I plan to serve as the knowledge base for my RAG system are latest scientific papers from arxiv. The model that I am using (Llama 2) was trained beteween Janurary 2023 and July 2023, so the selected documents that I have chosen have been created after then. The documents are all from 2024 and are April Fools documents so there are for sure no replicates of this in the original training set.\n",
        "\n",
        "https://arxiv.org/pdf/2403.19993\n",
        "\n",
        "https://arxiv.org/pdf/2403.19749\n",
        "\n",
        "https://arxiv.org/pdf/2403.20219\n",
        "\n",
        "https://arxiv.org/pdf/2403.20143\n",
        "\n",
        "https://arxiv.org/pdf/2403.20314\n",
        "\n",
        "The promps that I plan to use are:\n",
        "1. Why is FLAMINGO is the perfect name for an array of Cherenkov telescopes? (REAL)\n",
        "2. How do different messengers affect a person's personality? (REAL)\n",
        "3. What is the Universe of the birthday achieved by solving the Einstein-Boltzmann equations using CLASS? (REAL)\n",
        "4. What is Elon Musk's Goldfish name? (FAKE)"
      ],
      "metadata": {
        "id": "HfGxCZi9qFPn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPpiHVgj4CmG"
      },
      "source": [
        "## Installing dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zjLkm65J_S0v"
      },
      "outputs": [],
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0 > null\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ > null\n",
        "!pip install langchain > null\n",
        "!pip install chromadb > null\n",
        "!pip install sentence_transformers > null # ==2.2.2\n",
        "!pip install unstructured > null\n",
        "!pip install pdf2image > null\n",
        "!pip install pdfminer.six > null\n",
        "!pip install unstructured-pytesseract > null\n",
        "!pip install unstructured-inference > null\n",
        "!pip install faiss-gpu > null\n",
        "!pip install pikepdf > null\n",
        "!pip install pypdf > null\n",
        "!pip install accelerate > null\n",
        "!pip install pillow_heif > null\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes > null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30AIKqF9rTuv"
      },
      "source": [
        "Important\n",
        "------\n",
        "Restart the kernel after installing the packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "qEb7_aQ50Y-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X22Ccat1oXE2"
      },
      "source": [
        "# Imports\n",
        "Next, we import the necessary Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PatG1bZXGOws",
        "outputId": "a9bcbb96-d828-4ee7-e600-cffe9fced11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, BitsAndBytesConfig\n",
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "import locale\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# you need to define your private User Access Token from Huggingface\n",
        "# to be able to access models with accepted licence\n",
        "HUGGINGFACE_UAT=\"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\"\n",
        "login(HUGGINGFACE_UAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N_hQ_IfuoWT6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "7346ed469c244b51ab00a0aa182e0baa",
            "f32456de487341069da5d5225d30b679",
            "0cc99f40cc9c400f89164ee6a280b990",
            "1d94113e5d0a41608c6931205a81d9da",
            "7a5ed51bea3c48e78b42bead880ec8c9",
            "30dfa679dca74980b7024be5d86fc8e8",
            "27535a02c10e4b19a2b3b9229e23a600",
            "d05b556d50eb4a6a9c2e547092250526",
            "6ba06dded27a4e4d9d3fde58c4628c7d",
            "420e04a657e74f248349a6a50a7c6d60",
            "543b871b43804fc09be9d97a377434e1"
          ]
        },
        "outputId": "7aaf8555-528d-4b29-d841-4cd537fe95ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
            "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7346ed469c244b51ab00a0aa182e0baa"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"google/gemma-2b-it\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0 # For RAG we would like to have determenistic answers\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BX6bnnL45a2N"
      },
      "outputs": [],
      "source": [
        "template_gemma = \"\"\"\n",
        "user\n",
        "{text}\n",
        "model\n",
        "\"\"\"\n",
        "\n",
        "template_llama2 = \"\"\"\n",
        "user\n",
        "{text}\n",
        "model\n",
        "\"\"\"\n",
        "\n",
        "if \"gemma\" in model_name:\n",
        "  template=template_llama2\n",
        "else:\n",
        "  template=template_gemma\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Why is FLAMINGO is the perfect name for an array of Cherenkov telescopes?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))\n",
        "print(\"\\n\")\n",
        "\n",
        "text = \"How do different messengers affect a person's personality?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))\n",
        "print(\"\\n\")\n",
        "\n",
        "text = \"What is the Universe of the birthday achieved by solving the Einstein-Boltzmann equations using CLASS?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))\n",
        "print(\"\\n\")\n",
        "\n",
        "text = \"What is Elon Musk's Goldfish name?\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuuTfrBr2FOV",
        "outputId": "5a139c0e-05f2-4513-f847-522e7f1f8284"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user Why is FLAMINGO is the perfect name for an array of Cherenkov telescopes? model The perfect\n",
            "name for an array of Cherenkov telescopes is FLAMINGO.\n",
            "\n",
            "\n",
            "user How do different messengers affect a person's personality? model The personality of a person is\n",
            "affected by various messengers, including:  * ****Personal communication:** Personal communication\n",
            "involves direct interactions between a person and others, such as through phone calls, emails, and\n",
            "face-to-face conversations. * ****Media:** Media, such as television, radio, and social media,\n",
            "provides a platform for people to share information and interact with each other. *\n",
            "****Technology:** Technology, such as smartphones, laptops, and social media platforms, facilitates\n",
            "communication and allows people to connect with others in various ways. * ****Social groups:**\n",
            "Social groups, such as clubs, organizations, and communities, provide opportunities for people to\n",
            "engage with others and participate in shared activities. * ****Cultural influences:** Cultural\n",
            "influences shape the way people communicate and interact with each other, including through\n",
            "language, traditions, and values.  By interacting with these messengers, people can develop and\n",
            "maintain their personalities, learn about others, and create meaningful connections with others.\n",
            "\n",
            "\n",
            "user What is the Universe of the birthday achieved by solving the Einstein-Boltzmann equations using\n",
            "CLASS? model The Universe of the birthday achieved by solving the Einstein-Boltzmann equations using\n",
            "CLASS is a vast and infinite space that encompasses all of the known and unknown entities. It is a\n",
            "place where everything is possible, including the formation of new galaxies, the existence of exotic\n",
            "particles, and the ultimate fate of the universe itself.\n",
            "\n",
            "\n",
            "user What is Elon Musk's Goldfish name? model This information is not provided in the context.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2SNJZEtlDo1"
      },
      "source": [
        "## RAG on the web\n",
        "In this section, we download content from the internet, vectorise it and store the vectors, then search these vectors and generate the answer using the associated text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z8Y3YnRjJGEL"
      },
      "outputs": [],
      "source": [
        "web_loader = UnstructuredURLLoader(\n",
        "    urls=[\"https://arxiv.org/pdf/2403.19993\", \"https://arxiv.org/pdf/2403.19749\",\n",
        "          \"https://arxiv.org/pdf/2403.20219\", \"https://arxiv.org/pdf/2403.20143\",\n",
        "          \"https://arxiv.org/pdf/2403.20314\"], mode=\"elements\", strategy=\"fast\"\n",
        "    )\n",
        "web_doc = web_loader.load()\n",
        "updated_web_doc = filter_complex_metadata(web_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVjSIuPMmYvX",
        "outputId": "49d55eec-7fbc-4213-8e52-a2434db168ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "881"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
        "chunked_web_doc = text_splitter.split_documents(updated_web_doc)\n",
        "len(chunked_web_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R3hqEWvUcyBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95762ebe-bfc7-4373-a21d-ddc322a73ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings() # default model_name=\"sentence-transformers/all-mpnet-base-v2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA3xtLSQmh-v",
        "outputId": "c5252330-c87a-466e-dab4-540113215e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.73 s, sys: 39.7 ms, total: 2.77 s\n",
            "Wall time: 2.99 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Create the vectorized db with FAISS\n",
        "\n",
        "db_web = FAISS.from_documents(chunked_web_doc, embeddings)\n",
        "\n",
        "# Create the vectorized db with Chroma\n",
        "# from langchain.vectorstores import Chroma\n",
        "# db_web = Chroma.from_documents(chunked_web_doc, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxfNkw_PnItp",
        "outputId": "689f5949-de46-4ff6-db9c-89bb662d15be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 6.68 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "prompt_template_gemma = \"\"\"\n",
        "user\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}<end_of_turn>\n",
        "\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "prompt_template_llama2 = \"\"\"\n",
        "system\n",
        "\n",
        "Use the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "{context}user\n",
        "\n",
        "{question}assistant\n",
        "\"\"\"\n",
        "\n",
        "if \"gemma\" in model_name:\n",
        "  prompt_template=prompt_template_llama2\n",
        "else:\n",
        "  prompt_template=prompt_template_gemma\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_web = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_web.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n"
      ],
      "metadata": {
        "id": "p1S7nQqbgLRl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj-yQeYIH209",
        "outputId": "a3c1b708-4b0a-4d75-a34c-a60540793620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system  Use the following context to answer the question at the end. Do not use any other\n",
            "information. If you can't find the relevant information in the context, just say you don't have\n",
            "enough information to answer the question. Don't try to make up an answer.  Why FLAMINGO is the\n",
            "perfect name for an array of Cherenkov telescopes  Overall, using the name FLAMINGO for an array of\n",
            "Cherenkov telescopes can help to create a more inclu- sive and diverse community of astronomers and\n",
            "astro- physicists, promoting a more equitable and representa- tive field.  Finally, the name\n",
            "FLAMINGO can help to make ar- rays of Cherenkov telescopes more accessible and ap- proachable to\n",
            "people who may not have a background in astrophysics. By using a name that is fun and easy to\n",
            "remember, the project can help to break down barri-  In this paper, we have argued that FLAMINGO is\n",
            "the perfect name for an array of Cherenkov telescopes for several reasons. Firstly, the color pink,\n",
            "which is as- sociated with flamingos, has been shown to be the most suitable color for Cherenkov\n",
            "telescopes, providing the best characteristics to enhance sensitivity. Secondly, the keen visual\n",
            "ability of flamingos aligns well with the pri- mary purpose of Cherenkov telescopes to detect faint\n",
            "signals of high-energy cosmic rays and gamma rays. In addition, the name FLAMINGO has a fun and mem-\n",
            "orable quality that can help to increase public engage- ment and interest in the field of\n",
            "astrophysics. Using a name like FLAMINGO can also help to address diver- sity issues in the field of\n",
            "astrophysics by creating a more inclusive and welcoming community of astronomers and\n",
            "astrophysicists.  In conclusion, the name FLAMINGO represents a powerful branding strategy for any\n",
            "next-generation ar- ray of Cherenkov Telescopes, capturing the essence of the scientific goals of\n",
            "the project while creating a fun and approachable image. By using the name FLAMINGO and associated\n",
            "branding, the project can generate ex- citement and engagement among the public while also inspiring\n",
            "and supporting a more diverse flock of as- tronomers and astrophysicists. Visit https://flamingo.\n",
            "science. to stay up to date on the ongoing efforts of the FLAMINGO collaboration.user  Why is\n",
            "FLAMINGO is the perfect name for an array of Cherenkov telescopes?assistant The assistant provides\n",
            "the following answer:  Why is FLAMINGO is the perfect name for an array of Cherenkov\n",
            "telescopes?assistant?  The assistant provides the following answer:  The assistant provides the\n",
            "following answer:  The name FLAMINGO represents a powerful branding strategy for any next-generation\n",
            "ar- ray of Cherenkov Telescopes, capturing the essence of the scientific goals of the project while\n",
            "creating a fun and approachable image. By using the name FLAMINGO and associated branding, the\n",
            "project can generate ex- citement and engagement among the public while also inspiring and\n",
            "supporting a more diverse flock of as- tronomers and astrophysicists.\n",
            "\n",
            " \n",
            "\n",
            "system  Use the following context to answer the question at the end. Do not use any other\n",
            "information. If you can't find the relevant information in the context, just say you don't have\n",
            "enough information to answer the question. Don't try to make up an answer.  In this section, we\n",
            "cover the effects of different messen- gers on one’s personality. We go over the known messen- gers\n",
            "that we have observed in the past either as persistent sources or transients.  Cosmic rays are the\n",
            "only messenger with electric charge, and they therefore dictate your “electronic per- sonality”: the\n",
            "personality you project through text, so- cial media, and email for those older than z = 3 × 10−9.\n",
            "This electronic personality does not necessarily match the personality dictated by other messengers.\n",
            "For exam- ple, when the authors of this paper meet online friends in person, these friends are often\n",
            "upset that we are not “fun to be around” or “six-foot-two” like our profiles pro- claim. Although\n",
            "this was previously attributed to lying and insecurity, we can now see that it stems from a differ-\n",
            "ence in our cosmic ray astrological signs and our visible astrological signs.  Multi-Messenger\n",
            "Astrology  While this paper is not intended to describe the as- trological properties of messengers\n",
            "in an exhaustive way, it has left us exhausted, so we leave further analyses to smarter, harder-\n",
            "working individuals. In the meantime, the larger astrological society should be cognizant of the\n",
            "II. MULTI-MESSENGER SIGNALSuser  How do different messengers affect a person's personality?assistant\n",
            "Different messengers affect a person's personality through different means. Multi-messenger\n",
            "astrology allows us to understand these differences by analyzing the interactions between different\n",
            "messengers. This approach helps us to better understand ourselves and others, and to develop more\n",
            "effective communication strategies.\n",
            "\n",
            " \n",
            "\n",
            "system  Use the following context to answer the question at the end. Do not use any other\n",
            "information. If you can't find the relevant information in the context, just say you don't have\n",
            "enough information to answer the question. Don't try to make up an answer.  To constrain the value\n",
            "of π using cosmological ob- servations, we have modified the cosmological Einstein- Boltzmann solver\n",
            "CLASS [19] to take π as an input pa- rameter. Our version of CLASS is publicly available at\n",
            "https://github.com/EBHolm/class_public/tree/ varying-pi. Hence, the interested reader may try out\n",
            "the code for themselves, and even implement different values of π in their own, favourite model.\n",
            "The best-fit cosmology found from the baseline MCMC in the previous section is shown in Table I. By\n",
            "solv- ing the Einstein-Boltzmann equations using CLASS for these values, we find the age of the\n",
            "Universe to be 1022 nautical miles at the time of the pub- 6.712314 lication of this paper, i.e.\n",
            "April 1, 2024. By noting that on average, there are 365.242374 days in a year, we find that the age\n",
            "4015.63 Mpc corresponds to 13,097,255,367 years and 132 days ago from today. In order to find the\n",
            "correct birthday of the Universe, we must extrapolate our calendar back in time and account for all\n",
            "the leap years. There are three rules regarding leap years:  π enters the Einstein-Boltzmann\n",
            "equations in numer- ous places. Examples include the Fermi-Dirac initial distribution for neutrinos,\n",
            "the commonly occurring 8πG constant, primordial inflation potentials, changes to the Doppler\n",
            "corrections of the KHe term in the thermo- dynamics, in the computation of the Gauss-Legendre\n",
            "quadrature weights [12], and in the computation of the hyperspherical Bessel functions [20].\n",
            "Furthermore, a non- π value of π alters the definition of the Fourier transform, inherently\n",
            "impacting the differential equations solved and the final power spectrum quantities computed.  Our\n",
            "implementation of π in the Einstein-Boltzmann solver CLASS is rather rudimentary. For example, we\n",
            "did not rescale the Bessel functions, which have roots at π and are integral to the computation of\n",
            "the Cℓ coefficients, we did not take the effect on reaction rates during BBN into account, and we\n",
            "also did not rescale the periods of the trigonometric functions; ideally, one would rescale them as\n",
            "IMPLEMENTATION AND EFFECTS ON COSMOLOGICAL OBSERVABLESuser  What is the Universe of the birthday\n",
            "achieved by solving the Einstein-Boltzmann equations using CLASS?assistant By solving the Einstein-\n",
            "Boltzmann equations using CLASS for these values, we find the age of the Universe to be 1022\n",
            "nautical miles at the time of the pub- 6.712314 lication of this paper, i.e. April 1, 2024. By\n",
            "noting that on average, there are 365.242374 days in a year, we find that the age 4015.63 Mpc\n",
            "corresponds to 13,097,255,367 years and 132 days ago from today. In order to find the correct\n",
            "birthday of the Universe, we must extrapolate our calendar back in time and account for all the leap\n",
            "years. There are three rules regarding leap years:  π enters the Einstein-Boltzmann equations in\n",
            "numer- ous places. Examples include the Fermi-Dirac initial distribution for neutrinos, the commonly\n",
            "occurring 8πG constant, primordial inflation potentials, changes to the Doppler corrections of the\n",
            "KHe term in the thermo- dynamics, in the computation of the Gauss-Legendre quadrature weights [12],\n",
            "and in the computation of the hyperspherical Bessel functions [20]. Furthermore, a non- π value of π\n",
            "alters the definition of the Fourier transform, inherently impacting the differential equations\n",
            "solved and the final power spectrum quantities computed.\n",
            "\n",
            " \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:342: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.2\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system  Use the following context to answer the question at the end. Do not use any other\n",
            "information. If you can't find the relevant information in the context, just say you don't have\n",
            "enough information to answer the question. Don't try to make up an answer.  user  What is Elon\n",
            "Musk's Goldfish name?assistant The context does not provide information about Elon Musk's Goldfish\n",
            "name, so I cannot answer this question from the context.\n"
          ]
        }
      ],
      "source": [
        "query = \"Why is FLAMINGO is the perfect name for an array of Cherenkov telescopes?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "query = \"How do different messengers affect a person's personality?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "query = \"What is the Universe of the birthday achieved by solving the Einstein-Boltzmann equations using CLASS?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "query = \"What is Elon Musk's Goldfish name?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7346ed469c244b51ab00a0aa182e0baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f32456de487341069da5d5225d30b679",
              "IPY_MODEL_0cc99f40cc9c400f89164ee6a280b990",
              "IPY_MODEL_1d94113e5d0a41608c6931205a81d9da"
            ],
            "layout": "IPY_MODEL_7a5ed51bea3c48e78b42bead880ec8c9"
          }
        },
        "f32456de487341069da5d5225d30b679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30dfa679dca74980b7024be5d86fc8e8",
            "placeholder": "​",
            "style": "IPY_MODEL_27535a02c10e4b19a2b3b9229e23a600",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0cc99f40cc9c400f89164ee6a280b990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d05b556d50eb4a6a9c2e547092250526",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ba06dded27a4e4d9d3fde58c4628c7d",
            "value": 2
          }
        },
        "1d94113e5d0a41608c6931205a81d9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_420e04a657e74f248349a6a50a7c6d60",
            "placeholder": "​",
            "style": "IPY_MODEL_543b871b43804fc09be9d97a377434e1",
            "value": " 2/2 [00:21&lt;00:00,  8.78s/it]"
          }
        },
        "7a5ed51bea3c48e78b42bead880ec8c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30dfa679dca74980b7024be5d86fc8e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27535a02c10e4b19a2b3b9229e23a600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d05b556d50eb4a6a9c2e547092250526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba06dded27a4e4d9d3fde58c4628c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "420e04a657e74f248349a6a50a7c6d60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "543b871b43804fc09be9d97a377434e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
